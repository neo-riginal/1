{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30474,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Импорт библиотек","metadata":{}},{"cell_type":"code","source":"#импорт библиотек\nimport glob\nimport os\nimport pandas as pd\nimport numpy as np\nimport json\nimport docx\nfrom bs4 import BeautifulSoup as bs\nimport requests\nfrom tqdm.auto import tqdm, trange\nfrom pymystem3 import Mystem\nfrom nltk.stem.snowball import SnowballStemmer \nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem import *\nfrom nltk.corpus import stopwords\nimport string\nimport re\nimport pyLDAvis.sklearn\nimport pyLDAvis\nimport pyLDAvis.lda_model\nimport pymorphy2","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 1.1 Парсинг данных","metadata":{}},{"cell_type":"markdown","source":"Получим список компаний из docx документа","metadata":{}},{"cell_type":"code","source":"docx = docx.Document('Condidates.docx')\nparag = docx.paragraphs","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"company_docx = []\nfor i in parag:\n    company_docx.append(i.text)\ncompany_docx","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"В списке присутсвуют пустые строки, которые надо удалить","metadata":{}},{"cell_type":"code","source":"company_docx = list(filter(None, company_docx))\ncompany_docx","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Парсинг Json файла","metadata":{}},{"cell_type":"markdown","source":"Получаем список файлов, которые находятся в папке Data","metadata":{}},{"cell_type":"code","source":"list_json = glob.glob(\"Data/*.json\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Парсинг файлов","metadata":{}},{"cell_type":"code","source":"companies_json = {'name':[], 'raiting':[], 'date':[], \"text\":[], 'about': [], 'industries':[]}\nfor json_text in list_json:\n    with open(json_text, \"r\", encoding='utf-8') as f: #Чтение файлов\n        text = json.load(f)\n    for i in text['refs']: # Получение данных из полей \"refs\", в котором находится информация о содержании текста, и его даты\n        name = f.name[5:-5]\n\n        if text['info'] != None: # Получение данных из полей \"info\", в котором находится информация о содержании текста, и его даты\n            companies_json['industries'].append(text[\"info\"][\"industries\"])\n            companies_json['about'].append(text[\"info\"][\"about\"])\n            companies_json['raiting'].append(text[\"info\"][\"rate\"])\n        else:\n            companies_json['industries'].append(np.nan)\n            companies_json['about'].append(np.nan)\n            companies_json['raiting'].append(np.nan)\n\n        companies_json['name'].append(name) # Получение названии компании через название файла\n        if i != None:\n            companies_json['text'].append(i)\n\n            date = i[1]\n            date1 = date['day'] + \" \" + date[\"month\"] + \" \" + date[\"time\"]\n            companies_json['date'].append(date1)\n        else:\n            companies_json['text'].append(np.nan)\n            companies_json['date'].append(np.nan)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.DataFrame.from_dict(data=companies_json, orient=\"index\")\ndf = df.transpose()\ndf","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Заполнение данных об статьях комампний, которые указаны в docx файле","metadata":{}},{"cell_type":"markdown","source":"Вывод уникальных названий компаний в новый список.","metadata":{}},{"cell_type":"code","source":"import numpy as np\nuniq_comp = np.unique(companies_json['name'])\nprint(uniq_comp)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Вывод названий компаний, которые отсутсвуют в Json списке, в новый список","metadata":{}},{"cell_type":"code","source":"not_in_json = []\nfor comp in company_docx:\n    if comp not in uniq_comp:\n        not_in_json.append(comp)\nnot_in_json","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"На основе датасета с данными из Json добавляем информацию о компаниях, которые есть в спике docx документа, но нет в этом датасете. Если такие существуют, то находим информацию и компании на сайте Хабр и получаем текст и дату последних 10 статьей","metadata":{}},{"cell_type":"code","source":"import re\nfor name_company in not_in_json: #Перебор всех ссылок на профили компаний, которые есть в списке отсутствующих в Json файле\n    name_company1 = '%20'.join(name_company.split())\n    url = 'https://habr.com/ru/search/?q='+ name_company1 + \"&target_type=companies&order=relevance\"\n    page = requests.get(url)\n    soup = bs(page.text, 'html.parser')\n    link = soup.find('a', class_='tm-company-snippet__title')\n    description = \"\"\n    rate = \"\"\n\n    comp_name = name_company\n    company_is_real = False\n    if link: #Если профиль компании существует, то получаем информацию о компании оттуда\n        company_is_real = True\n        link = link.get(\"href\")\n        url = 'https://habr.com'+ link\n        page = requests.get(url)\n        soup = bs(page.text, 'html.parser')\n\n        rate = soup.find('span', class_='tm-votes-lever__score-counter tm-votes-lever__score-counter tm-votes-lever__score-counter_rating')\n        description = soup.find('span', class_='tm-company-profile__content')\n        sphera = soup.findAll('span', class_='tm-company-profile__categories-wrapper')\n        spheras = []\n        indastries = []\n        for i in sphera:\n            spheras.append(i.text.strip())\n        indastries.append(\", \".join(spheras))\n\n\n    url = 'https://habr.com/ru/search/?q='+ name_company1 + \"&target_type=posts&order=relevance\"\n    print(url)\n    page = requests.get(url)\n    soup = bs(page.text, 'html.parser')\n    pages = soup.find_all('a', class_='tm-pagination__page')\n    if len(pages) != 0:\n        pages = int(pages[1].text)\n    else:\n        pages = 1\n\n    for num in range(pages+1):\n        url = 'https://habr.com/ru/search/page' + str(num) + '?q='+ name_company1 + \"&target_type=posts&order=relevance\"\n        page = requests.get(url)\n        soup = bs(page.text, 'html.parser')\n\n        links = soup.find_all('a', class_='tm-title__link') \n\n        for link in links:\n            url = \"https://habr.com\" + link.get('href')\n            page = requests.get(url)\n            soup = bs(page.text, \"html.parser\")\n\n            date = soup.find('span', class_=\"tm-article-datetime-published\")\n            text = soup.find('div', class_=\"article-formatted-body article-formatted-body article-formatted-body_version-2\")\n\n            if text:\n                companies_json[\"text\"].append(text.text)\n            else:\n                companies_json[\"text\"].append(np.nan)\n\n            if description and company_is_real:\n                companies_json[\"about\"].append(description.text)\n            else:\n                companies_json[\"about\"].append(np.nan)\n\n            if rate and company_is_real:\n                number = float(re.findall(r'\\d+.\\d+', rate.text)[0])\n                companies_json[\"raiting\"].append(number)\n            else:\n                companies_json[\"raiting\"].append(np.nan)\n\n            companies_json[\"name\"].append(comp_name)\n\n            if date:\n                date_split = date.text.split()\n\n                if (date_split[1] == 'янв'):\n                    date_split[1] = 'январь'\n                if (date_split[1] == 'фев'):\n                    date_split[1] = 'февраль'\n                if (date_split[1] == 'мар'):\n                    date_split[1] = 'март'\n                if (date_split[1] == 'апр'):\n                    date_split[1] = 'апрель'\n                if (date_split[1] == 'июн'):\n                    date_split[1] = 'июнь'          \n                if (date_split[1] == 'июн'):\n                    date_split[1] = 'июнь' \n                if (date_split[1] == 'июл'):\n                    date_split[1] = 'июль' \n                if (date_split[1] == 'авг'):\n                    date_split[1] = 'август' \n                if (date_split[1] == 'сен'):\n                    date_split[1] = 'сентябрь' \n                if (date_split[1] == 'окт'):\n                    date_split[1] = 'октябрь'\n                if (date_split[1] == 'ноя'):\n                    date_split[1] = 'ноябрь' \n                if (date_split[1] == 'дек'):\n                    date_split[1] = 'декабрь'\n                if(len(date_split) == 5):\n                            date_f = date_split[0] + \" \" + date_split[1] + \" \" + date_split[4]\n                elif(len(date_split) == 4):\n                            date_f = date_split[0] + \" \" + date_split[1] + \" \" + date_split[3]        \n                companies_json[\"date\"].append(date_f)\n            else:\n                companies_json[\"date\"].append(np.nan)\n\n            if (company_is_real):\n                companies_json[\"industries\"].append(indastries)\n            else:\n                companies_json[\"industries\"].append(np.nan)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.DataFrame.from_dict(data=companies_json, orient=\"index\")\ndf = df.transpose()\ndf","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Сохранение данных в csv формате","metadata":{}},{"cell_type":"code","source":"# Запись файла\nFILE_NAME = \"habr.csv\"\ndf = pd.DataFrame.from_dict(data=companies_json, orient=\"index\")\ndf = df.transpose()\ndf.columns=['name', 'raiting', 'date', \"text\", 'about', 'industries']\ndf.to_csv(FILE_NAME, index=False)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Чтение файла\ndf = pd.read_csv('habr.csv')\ndf.info()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"__Вывод__: получили данные статьей из Json файла и информацию об компаниях и их статьях с сайта Habr. Также заполнили недостоющую информацию о компаниях в Json файле, у которых остутсвовала информация. Сформировали все данные с единый датасет","metadata":{}},{"cell_type":"markdown","source":"## 1.2 Формирование структуры набора данных","metadata":{}},{"cell_type":"markdown","source":"Проверка на пустые значения","metadata":{}},{"cell_type":"code","source":"df.isnull().sum()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Определим нужные атрибуты, для номинации премии Рунета.\nНазвание для определения победителя, текст определения сферы в номинации и рейтинг. Остальные артибуты удалим","metadata":{}},{"cell_type":"code","source":"df = df.drop(['about'], axis=1)\ndf = df.drop(['date'], axis=1)\ndf = df.drop(['industries'], axis=1)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Удалим статьи, которые содержат пустые значения","metadata":{}},{"cell_type":"code","source":"df = df.dropna()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"__Вывод:__ определили нужные атрибуты, которые будем использовать для последующей обработки. Удалили статьи, у которых были пустые поля.","metadata":{}},{"cell_type":"markdown","source":"### 1.3 Предварительная обработка текстовых данных ","metadata":{}},{"cell_type":"markdown","source":"Создадим функции для удаления лишних слов и символов","metadata":{}},{"cell_type":"code","source":"import string\nfrom pymystem3 import Mystem\nfrom tqdm.auto import tqdm, trange\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem.snowball import SnowballStemmer as snow \n\ndef remove_punctuation(text): #Функция удаления знаков препинания\n    return \"\".join([ch if ch not in string.punctuation else ' ' for ch in text])\ndef remove_numbers(text): #Функция удаления чисел\n    return ''.join([i if not i.isdigit() else ' ' for i in text])\ndef remove_multiple_spaces(text): #Функция удаления пробелов\n    return re.sub(r'\\s+', ' ', text, flags=re.I)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"prep_text = [remove_multiple_spaces(remove_numbers(remove_punctuation(text.lower()))) for text in tqdm(df['text'])]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from nltk.corpus import stopwords\nrussian_stopwords = stopwords.words(\"russian\") #создание списка стоп-слов\nrussian_stopwords.extend(['…', '«', '»', '...', 'т.д.', 'т', 'д', 'это']) #Дополнение стоп-слов","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Токенизация","metadata":{}},{"cell_type":"code","source":"sw_texts_list = [] #создание списка текста с удаленными стоп словами\nfor text in tqdm(prep_text):\n    tokens = word_tokenize(text)    \n    tokens = [token for token in tokens if token not in russian_stopwords and token != ' ']\n    text = \" \".join(tokens)\n    sw_texts_list.append(text)\n\ndf['text_sw'] = sw_texts_list","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Стемминг","metadata":{}},{"cell_type":"code","source":"stemmer = snow(\"russian\") \nstemmed_texts_list = [] #создание списка стемминга слов\nfor text in tqdm(df[\"text_sw\"]):\n    tokens = word_tokenize(text)    \n    stemmed_tokens = [stemmer.stem(token) for token in tokens if token not in russian_stopwords]\n    text = \" \".join(stemmed_tokens)\n    stemmed_texts_list.append(text)\n\ndf['text_stem'] = stemmed_texts_list","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Лемитизация","metadata":{}},{"cell_type":"code","source":"from pymystem3 import Mystem\nfrom tqdm.auto import tqdm, trange\nmystem = Mystem() \nlemm_texts_list = [] # создание и заполенение списка с текстом статей, где словам присвоили начальную форму.\nfor text in tqdm(df[\"text_sw\"]):\n    try:\n        text_lem = mystem.lemmatize(text)\n        tokens = [token for token in text_lem if token != ' ' and token not in russian_stopwords]\n        text = \" \".join(tokens)\n        lemm_texts_list.append(text)\n    except Exception as e:\n        print(e)\n    \ndf['text_lemm'] = lemm_texts_list","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['text_sw'].drop\ndf['text_stem'].drop","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Выделение частей речи","metadata":{}},{"cell_type":"markdown","source":"Процесс преобразования предложения в формы, то есть в список слов или кортежей (каждый кортеж имеет вид [слово, часть речи]).","metadata":{}},{"cell_type":"code","source":"morph = pymorphy2.MorphAnalyzer(lang='ru')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"__pymorphy2__ умеет разбирать не только словарные слова; для несловарных слов автоматически задействуется предсказатель.","metadata":{}},{"cell_type":"code","source":"ps_text_list = []\nfor text in tqdm(df['text_lemm']):\n    try:\n        words = word_tokenize(text)\n        text_list = []\n        for i in range(len(words)):\n            p = morph.parse(words[i])[0]\n            text_list.append(\"(\" + words[i] + \", \" + str(p.tag.POS) + \")\")\n        ps_text_list.append(text_list)\n    except Exception as e:\n        print(e)\ndf['text_ps'] = ps_text_list","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.to_csv('habr_f.csv', index=False)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 1.4 Поиск n-грамм. Векторизация текстов","metadata":{}},{"cell_type":"markdown","source":"__Мешок слов__ - решает проблему размерности по одной оси. Количество строк определяется количеством документов. Однако, этот метод не учитывает важность того или иного токена, ведь одно слово может повторятся по несколько раз.\n\n__TF-IDF__ - это способ векторизации текста, отражающий важность слова в документе, а не только частоту его появления.\n\n__Word Embeddings__ - векторное представление слов. Векторы можно складывать, вычитать, сравнивать.","metadata":{}},{"cell_type":"code","source":"df.isnull().sum()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Чтение файла\ndf = pd.read_csv('habr_f.csv')\ndf.info()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Векторизация мешок слов","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vectorizer = CountVectorizer(analyzer='word', stop_words=russian_stopwords, ngram_range=(1, 3), min_df=2)\ncount_matrix = vectorizer.fit_transform(df['text_lemm'])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"count_matrix.shape","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vectorizer.get_feature_names_out()[:50]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.decomposition import LatentDirichletAllocation\nRAND = 10\nlda_model = LatentDirichletAllocation(n_components=11)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lda_model.fit(count_matrix)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Векторизация TF-IDF","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.decomposition import LatentDirichletAllocation","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tfidf_vectorizer = TfidfVectorizer(max_df=0.8, max_features=10000,\n                                 min_df=0.01, stop_words=russian_stopwords,\n                                 ngram_range=(1,3))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\ntfidf_matrix = tfidf_vectorizer.fit_transform(df['text_lemm'])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tfidf_matrix.shape","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tfidf_vectorizer.get_feature_names_out()[:100]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 1.4.4 Векторизация Word Embeddings","metadata":{}},{"cell_type":"code","source":"import nltk","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from gensim.models import Word2Vec","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Bigger_list=[]\nfor i in df['text_lemm']:\n     li = list(word_tokenize(i))\n     Bigger_list.append(li)\nModel= Word2Vec(Bigger_list,min_count=1,vector_size=300,workers=4)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Model.wv.key_to_index","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Добавление ключевых слов, биграмм и триграмм","metadata":{}},{"cell_type":"code","source":"for i in range(len(df)):\n    tfidf_vectorizer = TfidfVectorizer(stop_words=russian_stopwords, ngram_range=(1,1))\n    tfidf_matrix= tfidf_vectorizer.fit_transform([df.loc[i, 'text_lemm']]).toarray()\n    df.loc[i, 'keyword'] = str(dict(zip(tfidf_vectorizer.get_feature_names_out(), tfidf_matrix[0])))\n    \n    tfidf_vectorizer = TfidfVectorizer(stop_words=russian_stopwords, ngram_range=(2,2))\n    tfidf_matrix= tfidf_vectorizer.fit_transform([df.loc[i, 'text_lemm']]).toarray()\n    df.loc[i, 'bigrams'] = str(dict(zip(tfidf_vectorizer.get_feature_names_out(), tfidf_matrix[0])))\n    \n    tfidf_vectorizer = TfidfVectorizer(stop_words=russian_stopwords, ngram_range=(3,3))\n    tfidf_matrix= tfidf_vectorizer.fit_transform([df.loc[i, 'text_lemm']]).toarray()\n    df.loc[i, 'trigrams'] = str(dict(zip(tfidf_vectorizer.get_feature_names_out(), tfidf_matrix[0])))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.to_csv('dataset.csv', index=False )","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 1.5 Разведочный анализ","metadata":{}},{"cell_type":"code","source":"with open('Target.json', \"r\",encoding=\"utf-8\") as my_file: #Открытие json файла\n    target_json = my_file.read() #чтение","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import json","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"targets = json.loads(target_json) #загрузка","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"namecomp = []\nnominat = []\nfor i in range(len(targets['list'])):\n    namecomp.append(targets['list'][i]['Сompany'])\n    nominat.append(targets['list'][i]['Nominations'])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"targets = pd.DataFrame()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"targets['namecompany'] = namecomp\ntargets['nomination'] = nominat","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"targets","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"targetscomp = []\nfor i in range(len(df['name'])):\n        k = 0\n        for j in range(len(targets['namecompany'])):\n            if(df['name'][i].lower().strip() == targets['namecompany'][j].lower().strip()):\n                targetscomp.append(targets['nomination'][j])\n            else:\n                k = k + 1\n        if(k == len(targets['namecompany'])):\n            targetscomp.append('Нет номинации')   ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(targetscomp)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['target'] = targetscomp","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.to_csv('dataset.csv', index=False)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn import preprocessing\nlabel_encoder = preprocessing.LabelEncoder()\ndf['target2']= label_encoder.fit_transform(df['target'])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.countplot(df['target2'])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns_plot = sns.distplot(df['target2'])\nfig = sns_plot.get_figure()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns_plot = sns.boxplot(df['target2'])\nfig = sns_plot.get_figure()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"stats.probplot(df['target2'], dist = \"norm\", plot=pylab)\npylab.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.set(rc={'figure.figsize':(20,10)})\nsns.scatterplot(data=df, x=\"target\", y=\"raiting\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv('dataset.csv')","metadata":{},"execution_count":null,"outputs":[]}]}